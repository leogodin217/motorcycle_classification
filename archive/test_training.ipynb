{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python setup\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torch import optim, cuda\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up paths\n",
    "base_path = Path('./clean_data/').absolute()\n",
    "raw_base_path = base_path / 'motorcycles'\n",
    "raw_train_path = str(raw_base_path / 'train')\n",
    "raw_val_path = str(raw_base_path / 'val')\n",
    "raw_test_path = str(raw_base_path / 'test')\n",
    "square_base_path = base_path / 'square_motorcycles'\n",
    "square_train_path = str(square_base_path / 'train')\n",
    "square_val_path = str(square_base_path / 'val')\n",
    "square_test_path = str(square_base_path / 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_model(num_classes):\n",
    "    '''\n",
    "    Creates a resnet-50 pretrained model and replaces the classifier with a new classifier\n",
    "    '''\n",
    "    model = models.resnet34(pretrained=True)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    num_inputs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(nn.Linear(num_inputs, 256),\n",
    "                                       nn.ReLU(),\n",
    "                                       # Get rid of dropout. I will re-evaluate later\n",
    "                                       #nn.Dropout(0.4),\n",
    "                                       nn.Linear(256, num_classes),\n",
    "                                       nn.LogSoftmax(dim=1))\n",
    "    # Move to the GPU\n",
    "    model = model.to('cuda')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, dataloader, criterion, clear_cuda_cache=True):\n",
    "    '''\n",
    "    Performs a forward pass getting loss and accuracy, without modifying the model\n",
    "    model: A pytorch NN \n",
    "    data A pytorch Dataloader\n",
    "    clear_cuda_cache: Do we want to clear cuda memory when possible?\n",
    "    '''\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for data, target in dataloader:\n",
    "            data = data.to('cuda')\n",
    "            target = target.to('cuda')\n",
    "            result = model(data)\n",
    "            loss = criterion(result, target)\n",
    "            batch_loss = loss.item() * data.size(0)\n",
    "            total_loss += batch_loss\n",
    "            if clear_cuda_cache is True:\n",
    "                data = None\n",
    "                target = None\n",
    "                cuda.empty_cache()\n",
    "    mean_loss = total_loss / len(dataloader.dataset)\n",
    "    return({'mean_loss': mean_loss})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, dataloader, clear_cuda_cache=True):\n",
    "    '''\n",
    "    Performs a forward pass getting loss and accuracy, without modifying the model\n",
    "    model: A pytorch NN \n",
    "    data A pytorch Dataloader\n",
    "    clear_cuda_cache: Do we want to clear cuda memory when possible?\n",
    "    '''\n",
    "    total_images = len(dataloader.dataset)\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for data, target in dataloader:\n",
    "            data = data.to('cuda')\n",
    "            target = target.to('cuda')\n",
    "            result = model(data)\n",
    "            _, predicted = torch.max(result.data, 1)\n",
    "            # Get accurate images\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "            if clear_cuda_cache is True:\n",
    "                data = None\n",
    "                target = None\n",
    "                cuda.empty_cache()\n",
    "    return correct_predictions / total_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/ for hyperparameters\n",
    "# hyper parameters are set to the research recommendations\n",
    "def basic_train_model(data, dataloaders, epochs, clear_cuda_cache=True, name='basic model',\n",
    "                      alpha=.001, beta1=0.9, beta2=0.999, epsilon=10e-8, weight_decay=0):\n",
    "    '''\n",
    "    Very-early training function. Not much here, but the basics to train the\n",
    "    model and report loss and cuda memory\n",
    "    data: A pytorch dataset with train and val data\n",
    "    dataloader: A Pytorch dataloader with train and validation datasets\n",
    "    clear_cuda_cache: Boolean telling us to clear the cuda cache when possible\n",
    "    name: String with a name to give the model.\n",
    "    '''\n",
    "    start_time = datetime.now()\n",
    "    results = []\n",
    "    cuda_memory = []\n",
    "    num_classes = len(data['train'].classes)\n",
    "\n",
    "    model = get_model(num_classes)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=alpha, betas=(beta1, beta2), \n",
    "                           eps=epsilon, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch: {epoch + 1}')\n",
    "        train_loss = 0.0\n",
    "        for data, targets in dataloaders['train']:\n",
    "             #Get cuda memory\n",
    "            cuda_memory.append({\n",
    "                'name': name,\n",
    "                'timestamp': datetime.now(),\n",
    "                'cuda_memory': cuda.memory_allocated()})\n",
    "            data = data.to('cuda')\n",
    "            targets = targets.to('cuda')\n",
    "            cuda_memory.append({\n",
    "                'name': name,\n",
    "                'timestamp': datetime.now(),\n",
    "                'cuda_memory': cuda.memory_allocated()})\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out, targets)\n",
    "            loss.backward()\n",
    "            # Get loss for the batch\n",
    "            batch_loss = loss.item() * data.size(0)\n",
    "            train_loss += batch_loss\n",
    "            optimizer.step()\n",
    "            #Get cuda memory\n",
    "            cuda_memory.append({\n",
    "                'name': name,\n",
    "                'timestamp': datetime.now(),\n",
    "                'cuda_memory': cuda.memory_allocated()})\n",
    "            # Clear the batch from cuda memory. It is no longer needed\n",
    "            if clear_cuda_cache is True:\n",
    "                data = None\n",
    "                targets = None\n",
    "                cuda.empty_cache()\n",
    "        \n",
    "        mean_train_loss = train_loss / len(dataloaders['train'].dataset)\n",
    "\n",
    "        \n",
    "        # Get validation loss\n",
    "        validation_results = forward_pass(model, dataloaders['val'], criterion, clear_cuda_cache=clear_cuda_cache)\n",
    "        # Get test accuracy\n",
    "        test_accuracy = get_accuracy(model, dataloaders['test'])\n",
    "        \n",
    "        print(f'Train_loss: {mean_train_loss}, Val loss: {validation_results[\"mean_loss\"]}, Test Accuracy: {test_accuracy}')\n",
    "        results.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': mean_train_loss,\n",
    "            'val_loss': validation_results['mean_loss'],\n",
    "            'test_accuracy': test_accuracy})\n",
    "        \n",
    "    end_time = datetime.now()\n",
    "    return {'model': model, 'name': name, 'results': results, 'cuda_memory': cuda_memory, \n",
    "            'run_time': end_time - start_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "\n",
    "basic_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize using same mean, std as imagenet\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize using same mean, std as ResNet\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "basic_data = {\n",
    "    'train': datasets.ImageFolder(root=raw_train_path, transform = basic_transforms['train'] ),\n",
    "    'valid': datasets.ImageFolder(root=raw_val_path, transform = basic_transforms['valid']),\n",
    "    'test': datasets.ImageFolder(root=raw_val_path, transform = basic_transforms['valid'])\n",
    "}\n",
    "\n",
    "basic_dataloaders = {\n",
    "    'train': DataLoader(basic_data['train'], batch_size=batch_size, shuffle=True),\n",
    "    'val': DataLoader(basic_data['valid'], batch_size=batch_size, shuffle=True),\n",
    "    'test': DataLoader(basic_data['test'], batch_size=batch_size, shuffle=True),\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "complex_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize using same mean, std as imagenet\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize using same mean, std as ResNet\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "complex_data = {\n",
    "    'train': datasets.ImageFolder(root=raw_train_path, transform = complex_transforms['train'] ),\n",
    "    'valid': datasets.ImageFolder(root=raw_val_path, transform = complex_transforms['valid']),\n",
    "     'test': datasets.ImageFolder(root=raw_val_path, transform = complex_transforms['valid'])\n",
    "}\n",
    "\n",
    "complex_dataloaders = {\n",
    "    'train': DataLoader(complex_data['train'], batch_size=batch_size, shuffle=True),\n",
    "    'val': DataLoader(complex_data['valid'], batch_size=batch_size, shuffle=True),\n",
    "    'test': DataLoader(complex_data['test'], batch_size=batch_size, shuffle=True)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train_loss: 6.005568485731266, Val loss: 5.898532177510046, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 2\n",
      "Train_loss: 5.89909698377195, Val loss: 5.8989420957440375, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 3\n",
      "Train_loss: 5.897452520611072, Val loss: 5.899466225153494, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 4\n",
      "Train_loss: 5.896954308250636, Val loss: 5.900050899694406, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 5\n",
      "Train_loss: 5.896485316892904, Val loss: 5.900623472149889, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 6\n",
      "Train_loss: 5.895924657466972, Val loss: 5.9011680993103734, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 7\n",
      "Train_loss: 5.895558754706414, Val loss: 5.901761434657779, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 8\n",
      "Train_loss: 5.89508610677037, Val loss: 5.902333010405606, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 9\n",
      "Train_loss: 5.894752823019834, Val loss: 5.902898572591368, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 10\n",
      "Train_loss: 5.894302387386366, Val loss: 5.9034855299814994, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 11\n",
      "Train_loss: 5.893984199345344, Val loss: 5.904058098966551, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 12\n",
      "Train_loss: 5.89353045313504, Val loss: 5.904564112034427, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 13\n",
      "Train_loss: 5.893212231610004, Val loss: 5.905105206990693, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 14\n",
      "Train_loss: 5.8928851510830444, Val loss: 5.905618644003611, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 15\n",
      "Train_loss: 5.892615257531055, Val loss: 5.906132536337018, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 16\n",
      "Train_loss: 5.892374722018198, Val loss: 5.906675829463998, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 17\n",
      "Train_loss: 5.89206479274715, Val loss: 5.907191963695543, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 18\n",
      "Train_loss: 5.891793672068066, Val loss: 5.907735406745122, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 19\n",
      "Train_loss: 5.891596591984187, Val loss: 5.908248428694546, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 20\n",
      "Train_loss: 5.891391710125113, Val loss: 5.908786011575091, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 21\n",
      "Train_loss: 5.891122124129991, Val loss: 5.909253205100868, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 22\n",
      "Train_loss: 5.890872833167623, Val loss: 5.909732506958852, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 23\n",
      "Train_loss: 5.890734284999931, Val loss: 5.9101264556596025, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 24\n",
      "Train_loss: 5.890499204901012, Val loss: 5.910576162379902, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 25\n",
      "Train_loss: 5.890410655806682, Val loss: 5.911043387139555, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 26\n",
      "Train_loss: 5.890226044797464, Val loss: 5.911431664462693, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 27\n",
      "Train_loss: 5.890108970423824, Val loss: 5.911810633396998, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 28\n",
      "Train_loss: 5.889978346806044, Val loss: 5.91231103963727, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 29\n",
      "Train_loss: 5.889859978337282, Val loss: 5.912728732329789, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 30\n",
      "Train_loss: 5.889601107846621, Val loss: 5.913067873214877, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 31\n",
      "Train_loss: 5.889566991977171, Val loss: 5.913445764233452, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 32\n",
      "Train_loss: 5.8894914148377815, Val loss: 5.913881966120291, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 33\n",
      "Train_loss: 5.889339546233376, Val loss: 5.914332761861698, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 34\n",
      "Train_loss: 5.8892542946943225, Val loss: 5.91469020787979, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 35\n",
      "Train_loss: 5.889101240507803, Val loss: 5.915131712584516, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 36\n",
      "Train_loss: 5.889043742875594, Val loss: 5.915440843962513, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 37\n",
      "Train_loss: 5.8888753052650715, Val loss: 5.915884054036813, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 38\n",
      "Train_loss: 5.8888182720685345, Val loss: 5.91626055618908, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 39\n",
      "Train_loss: 5.8888155902470665, Val loss: 5.916585944451967, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 40\n",
      "Train_loss: 5.888643904687214, Val loss: 5.916898930784223, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 41\n",
      "Train_loss: 5.888659220523115, Val loss: 5.9172507762214845, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 42\n",
      "Train_loss: 5.888537919102782, Val loss: 5.917521105409536, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 43\n",
      "Train_loss: 5.888471693924407, Val loss: 5.917812533427187, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 44\n",
      "Train_loss: 5.88845618524539, Val loss: 5.918130317976728, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 45\n",
      "Train_loss: 5.88845046878016, Val loss: 5.91851034400418, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 46\n",
      "Train_loss: 5.888305344104147, Val loss: 5.91889858176441, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 47\n",
      "Train_loss: 5.888286635841598, Val loss: 5.919240709996119, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 48\n",
      "Train_loss: 5.888315936213817, Val loss: 5.919511454247665, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 49\n",
      "Train_loss: 5.888225152371609, Val loss: 5.919770806463871, Test Accuracy: 0.001455604075691412\n",
      "Epoch: 50\n",
      "Train_loss: 5.888187746387774, Val loss: 5.920152063800084, Test Accuracy: 0.001455604075691412\n"
     ]
    }
   ],
   "source": [
    "raw_results_clear_cache = basic_train_model(data=basic_data, dataloaders=basic_dataloaders, epochs=50, alpha=0.003,\n",
    "                                            name='Basic images with basic transforms with clear', clear_cuda_cache=True)\n",
    "\n",
    "#complex_results_clear_cache = basic_train_model(data=complex_data, dataloaders=complex_dataloaders, epochs=50, alpha=0.003,\n",
    "#                                            name='Basic images with complex transforms with clear', clear_cuda_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b52a3ad160>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame(raw_results_clear_cache['results'])\n",
    "results.set_index('epoch')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>6.005568</td>\n",
       "      <td>5.898532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.899097</td>\n",
       "      <td>5.898942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.897453</td>\n",
       "      <td>5.899466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.896954</td>\n",
       "      <td>5.900051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.896485</td>\n",
       "      <td>5.900623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.895925</td>\n",
       "      <td>5.901168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.895559</td>\n",
       "      <td>5.901761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.895086</td>\n",
       "      <td>5.902333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.894753</td>\n",
       "      <td>5.902899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.894302</td>\n",
       "      <td>5.903486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.893984</td>\n",
       "      <td>5.904058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.893530</td>\n",
       "      <td>5.904564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.893212</td>\n",
       "      <td>5.905105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.892885</td>\n",
       "      <td>5.905619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.892615</td>\n",
       "      <td>5.906133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.892375</td>\n",
       "      <td>5.906676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.892065</td>\n",
       "      <td>5.907192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.891794</td>\n",
       "      <td>5.907735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.891597</td>\n",
       "      <td>5.908248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.891392</td>\n",
       "      <td>5.908786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.891122</td>\n",
       "      <td>5.909253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.890873</td>\n",
       "      <td>5.909733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.890734</td>\n",
       "      <td>5.910126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.890499</td>\n",
       "      <td>5.910576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.890411</td>\n",
       "      <td>5.911043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.890226</td>\n",
       "      <td>5.911432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.890109</td>\n",
       "      <td>5.911811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.889978</td>\n",
       "      <td>5.912311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.889860</td>\n",
       "      <td>5.912729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.889601</td>\n",
       "      <td>5.913068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.889567</td>\n",
       "      <td>5.913446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.889491</td>\n",
       "      <td>5.913882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.889340</td>\n",
       "      <td>5.914333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.889254</td>\n",
       "      <td>5.914690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.889101</td>\n",
       "      <td>5.915132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.889044</td>\n",
       "      <td>5.915441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.888875</td>\n",
       "      <td>5.915884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.888818</td>\n",
       "      <td>5.916261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.888816</td>\n",
       "      <td>5.916586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.888644</td>\n",
       "      <td>5.916899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.888659</td>\n",
       "      <td>5.917251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.888538</td>\n",
       "      <td>5.917521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.888472</td>\n",
       "      <td>5.917813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.888456</td>\n",
       "      <td>5.918130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.888450</td>\n",
       "      <td>5.918510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.888305</td>\n",
       "      <td>5.918899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.888287</td>\n",
       "      <td>5.919241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.888316</td>\n",
       "      <td>5.919511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.888225</td>\n",
       "      <td>5.919771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>5.888188</td>\n",
       "      <td>5.920152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  test_accuracy  train_loss  val_loss\n",
       "0       1       0.001456    6.005568  5.898532\n",
       "1       2       0.001456    5.899097  5.898942\n",
       "2       3       0.001456    5.897453  5.899466\n",
       "3       4       0.001456    5.896954  5.900051\n",
       "4       5       0.001456    5.896485  5.900623\n",
       "5       6       0.001456    5.895925  5.901168\n",
       "6       7       0.001456    5.895559  5.901761\n",
       "7       8       0.001456    5.895086  5.902333\n",
       "8       9       0.001456    5.894753  5.902899\n",
       "9      10       0.001456    5.894302  5.903486\n",
       "10     11       0.001456    5.893984  5.904058\n",
       "11     12       0.001456    5.893530  5.904564\n",
       "12     13       0.001456    5.893212  5.905105\n",
       "13     14       0.001456    5.892885  5.905619\n",
       "14     15       0.001456    5.892615  5.906133\n",
       "15     16       0.001456    5.892375  5.906676\n",
       "16     17       0.001456    5.892065  5.907192\n",
       "17     18       0.001456    5.891794  5.907735\n",
       "18     19       0.001456    5.891597  5.908248\n",
       "19     20       0.001456    5.891392  5.908786\n",
       "20     21       0.001456    5.891122  5.909253\n",
       "21     22       0.001456    5.890873  5.909733\n",
       "22     23       0.001456    5.890734  5.910126\n",
       "23     24       0.001456    5.890499  5.910576\n",
       "24     25       0.001456    5.890411  5.911043\n",
       "25     26       0.001456    5.890226  5.911432\n",
       "26     27       0.001456    5.890109  5.911811\n",
       "27     28       0.001456    5.889978  5.912311\n",
       "28     29       0.001456    5.889860  5.912729\n",
       "29     30       0.001456    5.889601  5.913068\n",
       "30     31       0.001456    5.889567  5.913446\n",
       "31     32       0.001456    5.889491  5.913882\n",
       "32     33       0.001456    5.889340  5.914333\n",
       "33     34       0.001456    5.889254  5.914690\n",
       "34     35       0.001456    5.889101  5.915132\n",
       "35     36       0.001456    5.889044  5.915441\n",
       "36     37       0.001456    5.888875  5.915884\n",
       "37     38       0.001456    5.888818  5.916261\n",
       "38     39       0.001456    5.888816  5.916586\n",
       "39     40       0.001456    5.888644  5.916899\n",
       "40     41       0.001456    5.888659  5.917251\n",
       "41     42       0.001456    5.888538  5.917521\n",
       "42     43       0.001456    5.888472  5.917813\n",
       "43     44       0.001456    5.888456  5.918130\n",
       "44     45       0.001456    5.888450  5.918510\n",
       "45     46       0.001456    5.888305  5.918899\n",
       "46     47       0.001456    5.888287  5.919241\n",
       "47     48       0.001456    5.888316  5.919511\n",
       "48     49       0.001456    5.888225  5.919771\n",
       "49     50       0.001456    5.888188  5.920152"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
